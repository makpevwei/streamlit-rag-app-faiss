1. Clone or download the repository

# âš¡ Fast RAG â€“ Groq & Gemini

**Fast document Q&A chat with optional high-accuracy Docling processing**

A lightweight, speed-optimized **Retrieval-Augmented Generation** (RAG) application built with Streamlit that lets you:

- Upload PDF, DOCX, PPTX, CSV, TXT, MD, HTML files
- Chat with your documents using **Groq (Llama 3.3 70B)** or **Google Gemini 1.5 Flash**
- Choose between **very fast** default processing or **smarter but slower** Docling mode for complex layouts & tables

## âœ¨ Features

- âš¡ **Two speed modes**:
  - Fast mode (default): simple loaders + recursive text splitting (~2â€“10 s)
  - Docling mode: advanced layout/table understanding (~30â€“180 s)
- ğŸ¤– **Dual LLM support** â€” Groq Llama-3.3-70B or Gemini 1.5 Flash (auto-fallback)
- ğŸ§  **Chat history** preserved in session
- ğŸ“„ **Source citation** with file name + page + snippet preview
- ğŸ” API keys via `.env` **or** Streamlit Cloud **secrets**
- ğŸ› ï¸ Clean error handling & user feedback

## ğŸš€ Quick Start (Local)

1. Clone or download the repository

```bash
git clone https://github.com/YOUR-USERNAME/fast-rag-streamlit.git
cd fast-rag-streamlit

2. Install dependencies

```bash
pip install -r requirements.txt

Typical requirements.txt:

streamlit>=1.38
python-dotenv
langchain
langchain-core
langchain-community
langchain-huggingface
langchain-groq
langchain-google-genai
faiss-cpu
pypdf
docx2txt
unstructured
python-pptx
python-docx
langchain-docling          # only needed if using Docling mode

3. Create .env file in root folder

```bash
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GOOGLE_API_KEY=AIz...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# optional: only one is required

4. Run the application

```bash
streamlit run app.py

ğŸ“Š When to Use Which Mode

Mode,Speed,Best for,Table extraction,Layout awareness,Recommended
Fast (default),â˜…â˜…â˜…â˜…â˜…,"Most documents, speed critical",Basic,Low,Yes âœ…
Docling,â˜…â˜†â˜†â˜†â˜†,"Complex PDFs, tables, forms, multi-column",Excellent,High,Only if needed

ğŸ› ï¸ Tech Stack

Frontend â€” Streamlit
Embeddings â€” all-MiniLM-L6-v2 (Hugging Face)
Vector Store â€” FAISS (CPU)
LLMs â€” Groq (Llama-3.3-70B) â€¢ Google Gemini 1.5 Flash
Document loading â€” PyPDF / Unstructured / Docx2txt / python-pptx + Docling (optional)
Chunking â€” RecursiveCharacterTextSplitter (1000/200)

âš™ï¸ Configuration & Tips

Change model temperature, chunk size, retriever k, etc. directly in code
Want better table support without Docling? â†’ consider adding unstructured[local-inference] + paddle/tesseract
Deploying to Streamlit Community Cloud? â†’ add secrets in the app settings

ğŸ“ License
MIT
ğŸ™Œ Acknowledgments
Built with love using:

LangChain
Groq
Google Generative AI
Docling (optional powerhouse)
FAISS

Happy RAG-ing! ğŸš€
